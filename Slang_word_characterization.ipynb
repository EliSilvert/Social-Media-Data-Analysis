{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import statistics\n",
    "from nltk.util import ngrams\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import itertools\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import words\n",
    "from collections import Counter\n",
    "import operator\n",
    "import pprint\n",
    "import datetime\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "def notin(target, comparelist):\n",
    "    for element in comparelist:\n",
    "        if element in target:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def preprocess(words):\n",
    "    words = words.lower()\n",
    "    # weird thing with apostrophe showing up as this unicode string\n",
    "    words = re.sub(\"\\u00e2\\u0080\\u0099\", \"'\", words)\n",
    "    # filter out all remaining Unicode, TODO: maybe want to translate these into emojis?\n",
    "    words = words.encode(\"ascii\", \"ignore\").decode(\"UTF-8\")\n",
    "    return words\n",
    "\n",
    "def print_sorted_dict(d, first_x=None):\n",
    "    sorted_keys = sorted(d, key=d.get, reverse=True)\n",
    "    for i, key in enumerate(sorted_keys):\n",
    "        if first_x is not None and i >= first_x:\n",
    "            break\n",
    "        print(f\"Rank {i+1}:\", key, d[key])\n",
    "    return sorted_keys\n",
    "\n",
    "def is_contraction(word):\n",
    "    if \"'\" in word:\n",
    "        split_word = word.split(\"'\")\n",
    "        if len(split_word) > 1 and split_word[0] in english_words:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def exclude_word(word):\n",
    "    return (len(word) < 1 or word in english_words or word.isdigit() \n",
    "            or is_contraction(word) or word in standard_contractions)\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "def print_term_to_senders():\n",
    "    pp.pprint(term_to_senders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_contractions = set([\"aren't\", \"can't\", \"could've\", \"couldn't\", \"didn't\", \n",
    "                \"doesn't\", \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"he'd\", \"he'll\", \"he's\", \n",
    "                \"how'd\", \"how'll\", \"how's\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"isn't\", \"it'd\", \n",
    "                \"it'll\", \"it's\", \"let's\", \"might've\", \"mightn't\", \"must've\", \n",
    "                \"mustn't\", \"needn't\", \"o'clock\", \"she'd\", \"she'll\", \"she's\", \n",
    "                \"should've\", \"shouldn't\", \"so's\", \"that'd\", \"that's\", \"there'd\", \n",
    "                \"there's\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"wasn't\", \n",
    "                \"we'd\", \"we'll\", \"we're\", \"we've\", \"weren't\", \"what'll\", \"what're\", \"what's\",\n",
    "                \"what've\", \"when's\", \"when've\", \"where'd\", \"where's\", \"where've\", \"who'll\", \n",
    "                \"who's\", \"who've\", \"why's\", \"won't\", \"would've\", \n",
    "                \"wouldn't\", \"you'd\", \"you'll\", \"you're\", \"you've\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# sample use cases\n",
    "# 1. Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 2. Lemmatize Single Word with the appropriate POS tag\n",
    "# word = 'feet'\n",
    "# print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "\n",
    "# 3. Lemmatize a Sentence with the appropriate POS tag\n",
    "# sentence = \"The striped bats are hanging on their feet for best\"\n",
    "# print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract message dictionaries\n",
    "\n",
    "msg_dir = \"/home/lhuang21/Documents/SideProjects/Grapevine/facebook-lawrenceh1850/messages/inbox\"\n",
    "user_name = \"Lawrence Huang\"\n",
    "\n",
    "# list of message dictionaries of interest\n",
    "conv_dicts = {}\n",
    "friend_to_num_msgs = defaultdict(lambda: 0)\n",
    "\n",
    "for dir_name in os.listdir(msg_dir):\n",
    "    if not dir_name.startswith(\".\"):\n",
    "        file_path = os.path.join(msg_dir, dir_name, 'message_1.json')\n",
    "        \n",
    "        # make sure file exists\n",
    "        if os.path.exists(file_path):\n",
    "            json_dict = json.load(open(file_path, 'r'))\n",
    "\n",
    "            # only look at DM's with more than 10 messages\n",
    "            if len(json_dict['participants']) == 2 and len(json_dict['messages']) > 10:\n",
    "                for friend_dict in json_dict['participants']:\n",
    "                    friend_name = friend_dict[\"name\"]\n",
    "                    if friend_name != user_name:\n",
    "                        num_msgs = len(json_dict['messages'])\n",
    "                        friend_to_num_msgs[friend_name] = num_msgs\n",
    "\n",
    "# print counts\n",
    "sorted_names = print_sorted_dict(friend_to_num_msgs)\n",
    "\n",
    "plt.bar(list(range(len(sorted_names))), [friend_to_num_msgs[name] for name in sorted_names])\n",
    "    \n",
    "# top ten percentile of friends        \n",
    "top_ten_percentile = sorted_names[:len(sorted_names) // 10]\n",
    "\n",
    "for dir_name in os.listdir(msg_dir):\n",
    "    if not dir_name.startswith(\".\"):\n",
    "        file_path = os.path.join(msg_dir, dir_name, 'message_1.json')\n",
    "        \n",
    "        # make sure file exists\n",
    "        if os.path.exists(file_path):\n",
    "            json_dict = json.load(open(file_path, 'r'))\n",
    "\n",
    "            # only look at DM's with more than 10 messages\n",
    "            if len(json_dict['participants']) == 2 and len(json_dict['messages']) > 10:\n",
    "                for friend_dict in json_dict['participants']:\n",
    "                    friend_name = friend_dict[\"name\"]\n",
    "                    if friend_name != user_name and friend_name in top_ten_percentile:\n",
    "                        print(friend_name)\n",
    "                        conv_dicts[friend_name] = json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts number of words\n",
    "\n",
    "friend_to_words = defaultdict(lambda: 0)\n",
    "\n",
    "for friend_name, conv_dict in conv_dicts.items():\n",
    "    print(\"Friend name:\", friend_name)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    messages = conv_dict[\"messages\"]\n",
    "    for (index, msg) in enumerate(messages):\n",
    "        try:\n",
    "            content = msg['content']\n",
    "            tok_content = content.split()\n",
    "            friend_to_words[friend_name] += len(tok_content)\n",
    "        except KeyError:\n",
    "            pass\n",
    "_ = print_sorted_dict(friend_to_words, first_x=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  get monogram/bigram/trigrams\n",
    "\n",
    "from nltk.corpus import words\n",
    "\n",
    "english_words = set(words.words())\n",
    "\n",
    "term_to_senders = {}\n",
    "# {\n",
    "#     \"<term>\": {\n",
    "#         \"<friend_name>\": \n",
    "#             {\n",
    "#                  \"<friend_name>\": [<list of timestamps in ms>],\n",
    "#                  \"<user_name>\": [<list of timestamps in ms>]\n",
    "#             }\n",
    "#         ....\n",
    "#     },\n",
    "#     ....\n",
    "# }\n",
    "\n",
    "def add_term_timestamp(term_dict, term, friend_name, sender_name, timestamp_ms):\n",
    "    # figure out which person didn't send it\n",
    "    non_sender = friend_name\n",
    "    if friend_name == sender_name:\n",
    "        non_sender = user_name\n",
    "    \n",
    "    # term doesn't exist in dict\n",
    "    if term_dict.get(term) is None:\n",
    "        term_dict[term] = {\n",
    "            friend_name: {\n",
    "                sender_name: [timestamp_ms],\n",
    "                non_sender: []\n",
    "            }\n",
    "        }\n",
    "    else: # term exists in dict\n",
    "        if term_dict[term].get(friend_name) is None: # not yet found in this conversation\n",
    "            term_dict[term][friend_name] = { sender_name: [timestamp_ms], non_sender: [] }\n",
    "        else:\n",
    "            term_dict[term][friend_name][sender_name].append(timestamp_ms)\n",
    "\n",
    "for friend_name, conv_dict in conv_dicts.items():\n",
    "    print(\"Conversation with\", friend_name)\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    messages = conv_dict[\"messages\"]\n",
    "    for (index, msg) in enumerate(messages): \n",
    "        try:\n",
    "            sender_name = msg[\"sender_name\"]\n",
    "            content = msg[\"content\"]\n",
    "            timestamp_ms = msg[\"timestamp_ms\"]\n",
    "\n",
    "            if notin(content, [\"You are now connected on Messenger.\", \"Say hi to your new Facebook friend\"]):\n",
    "                content = preprocess(content)\n",
    "                \n",
    "#                 alternate ways of tokenizing\n",
    "#                 tok_content = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(content)]\n",
    "#                 tok_content = nltk.word_tokenize(content)\n",
    "\n",
    "                tok_content = content.split()\n",
    "\n",
    "                # monograms\n",
    "                for word in tok_content:\n",
    "                    word = word.strip(string.punctuation)\n",
    "                    if len(word) > 0:\n",
    "                        word = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "                        if not exclude_word(word):\n",
    "                            add_term_timestamp(term_to_senders, word, friend_name, sender_name, timestamp_ms)\n",
    "                        \n",
    "                # bigrams\n",
    "                for (tok1, tok2) in ngrams(tok_content, 2):\n",
    "                    tok1 = tok1.strip(string.punctuation)\n",
    "                    tok2 = tok2.strip(string.punctuation)\n",
    "                    if (exclude_word(tok1) and exclude_word(tok2)\n",
    "                        or (tok1.isdigit() or tok2.isdigit())):\n",
    "                        continue\n",
    "                    else:\n",
    "                        add_term_timestamp(term_to_senders, (tok1, tok2), friend_name, sender_name, timestamp_ms)\n",
    "                        \n",
    "                # trigrams\n",
    "                for (tok1, tok2, tok3) in ngrams(tok_content, 3):\n",
    "                    tok1 = tok1.strip(string.punctuation)\n",
    "                    tok2 = tok2.strip(string.punctuation)\n",
    "                    tok3 = tok3.strip(string.punctuation)\n",
    "                    num_stop = 0\n",
    "                    if exclude_word(tok1):\n",
    "                        num_stop += 1\n",
    "                    if exclude_word(tok2):\n",
    "                        num_stop += 1\n",
    "                    if exclude_word(tok3):\n",
    "                        num_stop += 1\n",
    "                        \n",
    "                    if num_stop >= 2:\n",
    "                        continue\n",
    "                    else:\n",
    "                        add_term_timestamp(term_to_senders, (tok1, tok2, tok3), friend_name, sender_name, timestamp_ms)\n",
    "        except KeyError:\n",
    "            # messages that are just photos don't have a content key\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "term_to_senders_original = copy.deepcopy(term_to_senders)\n",
    "# print(len(term_to_senders))\n",
    "# print(len(term_to_senders_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER term_to_senders, \n",
    "# TODO: sort by occurrernce, and see the cutoff filtering based on something besides frequency\n",
    "\n",
    "# term_to_senders schema\n",
    "# {\n",
    "#     \"<term>\": {\n",
    "#         \"<friend_name>\": \n",
    "#             {\n",
    "#                  \"<friend_name>\": [<list of timestamps in ms>],\n",
    "#                  \"<user_name>\": [<list of timestamps in ms>]\n",
    "#             }\n",
    "#         ....\n",
    "#     },\n",
    "#     ....\n",
    "# }\n",
    "\n",
    "term_to_senders = copy.deepcopy(term_to_senders_original)\n",
    "terms_to_delete = []\n",
    "term_to_count = defaultdict(lambda: 0)\n",
    "for term in term_to_senders:\n",
    "    num_usages = 0\n",
    "    for friend_name in term_to_senders[term]:\n",
    "        for user_name in term_to_senders[term][friend_name]:\n",
    "            num_usages += len(term_to_senders[term][friend_name][user_name])\n",
    "    if num_usages < 4:\n",
    "        terms_to_delete.append(term)\n",
    "    else:\n",
    "        term_to_count[term] += num_usages\n",
    "for term in terms_to_delete:\n",
    "     del term_to_senders[term]\n",
    "print(len(term_to_senders)) # should be 10465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINTS SLANG RANKED BY USAGE, ACROSS ALL USERS\n",
    "_ = print_sorted_dict(term_to_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MODIFY THESE\n",
    "# =================================\n",
    "term_examined = \"lol\"\n",
    "friend_name = \"Dan J Chong\"\n",
    "# =================================\n",
    "sample_dict = term_to_senders[term_examined][friend_name]\n",
    "\n",
    "other_mentions = [datetime.datetime.utcfromtimestamp(i / 1000).replace(tzinfo=datetime.timezone.utc) for i in sample_dict[friend_name]]\n",
    "me_mentions = [datetime.datetime.utcfromtimestamp(i / 1000).replace(tzinfo=datetime.timezone.utc) for i in sample_dict[user_name]]\n",
    "\n",
    "# This tutorial as reference: https://matplotlib.org/3.0.0/gallery/lines_bars_and_markers/timeline.html\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(25, 5))\n",
    "\n",
    "# Create the base line\n",
    "start = min(me_mentions + other_mentions)\n",
    "stop = max(me_mentions + other_mentions)\n",
    "ax.plot((start, stop), (0, 0), 'k', alpha=.5)\n",
    "\n",
    "# Iterate through releases annotating each one\n",
    "for i, date in enumerate(me_mentions):\n",
    "    ax.plot((date, date), (0, 1), c='r', alpha=.7, label=f\"{user_name}\" if i == 0 else \"_nolegend_\")\n",
    "for i, date in enumerate(other_mentions):\n",
    "    # Plot a line up to the text\n",
    "    ax.plot((date, date), (0, -1), c='b', alpha=.7, label=f\"{friend_name}\" if i == 0 else \"_nolegend_\")\n",
    "\n",
    "ax.set(title=f\"Mentions of {term_examined} with {friend_name}\")\n",
    "# Set the xticks formatting\n",
    "# TODO: make the month interval calculation dynamic based on difference between start and stop\n",
    "# format xaxis with x month intervals\n",
    "ax.get_xaxis().set_major_locator(mdates.MonthLocator(interval=1))\n",
    "ax.get_xaxis().set_major_formatter(mdates.DateFormatter(\"%b %Y\"))\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "plt.legend(loc='upper left');\n",
    "\n",
    "# Remove components for a cleaner look\n",
    "plt.setp((ax.get_yticklabels() + ax.get_yticklines() +\n",
    "          list(ax.spines.values())), visible=False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
