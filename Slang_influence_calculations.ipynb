{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "import pickle\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import datetime\n",
    "from datetime import timezone\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     8,
     27
    ]
   },
   "outputs": [],
   "source": [
    "# utility objects\n",
    "\n",
    "MILLISEC_PER_MONTH = 1000 * 60 * 60 * 24 * 30\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "def print_term_to_senders():\n",
    "    pp.pprint(term_to_senders)\n",
    "    \n",
    "standard_contractions = set([\"aren't\", \"can't\", \"could've\", \"couldn't\", \"didn't\", \n",
    "            \"doesn't\", \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"he'd\", \"he'll\", \"he's\", \n",
    "            \"how'd\", \"how'll\", \"how's\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"isn't\", \"it'd\", \n",
    "            \"it'll\", \"it's\", \"let's\", \"might've\", \"mightn't\", \"must've\", \n",
    "            \"mustn't\", \"needn't\", \"o'clock\", \"she'd\", \"she'll\", \"she's\", \n",
    "            \"should've\", \"shouldn't\", \"that'd\", \"that's\", \"there'd\", \n",
    "            \"there's\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"wasn't\", \n",
    "            \"we'd\", \"we'll\", \"we're\", \"we've\", \"weren't\", \"what'll\", \"what're\", \"what's\",\n",
    "            \"what've\", \"when's\", \"when've\", \"where'd\", \"where's\", \"where've\", \"who'll\", \n",
    "            \"who's\", \"who've\", \"why's\", \"won't\", \"would've\", \n",
    "            \"wouldn't\", \"you'd\", \"you'll\", \"you're\", \"you've\"])\n",
    "\n",
    "# add in all contractions without the apostrophes also\n",
    "new_contractions = set()\n",
    "for contraction in standard_contractions:\n",
    "    new_contractions.add(contraction.replace(\"'\", \"\"))\n",
    "\n",
    "standard_contractions = standard_contractions.union(new_contractions)\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     2,
     19,
     27
    ]
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def preprocess(words):\n",
    "    words = words.lower()\n",
    "    # weird thing with apostrophe showing up as this unicode string\n",
    "    words = re.sub(\"\\u00e2\\u0080\\u0099\", \"'\", words)\n",
    "    # filter out all remaining Unicode, TODO: maybe want to translate these into emojis?\n",
    "    words = words.encode(\"ascii\", \"ignore\").decode(\"UTF-8\")\n",
    "    return words\n",
    "\n",
    "def print_sorted_dict(d, first_x=None, actually_print=True):\n",
    "    sorted_keys = sorted(d, key=d.get, reverse=True)\n",
    "    for i, key in enumerate(sorted_keys):\n",
    "        if first_x is not None and i >= first_x:\n",
    "            break\n",
    "        if actually_print:\n",
    "            print(f\"Rank {i+1}:\", key, d[key])\n",
    "    return sorted_keys\n",
    "\n",
    "def is_contraction(word):\n",
    "    if \"'\" in word:\n",
    "        split_word = word.split(\"'\")\n",
    "        if len(split_word) > 1 and split_word[0] in english_words:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "terms_to_include = set([\"kys\"])\n",
    "def exclude_word(word):\n",
    "    return (len(word) < 1 or word in english_words or word.isdigit() \n",
    "            or is_contraction(word) or word in standard_contractions) and (word not in terms_to_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# load dictionary from disk\n",
    "\n",
    "pickled_dict_fname = \"term_to_senders_dict\"\n",
    "term_to_senders = pickle.load(open(pickled_dict_fname, \"rb\")) \n",
    "\n",
    "# {\n",
    "#     \"<term>\": {\n",
    "#         \"<friend_name>\": \n",
    "#             {\n",
    "#                  \"<friend_name>\": [<list of timestamps in ms>],\n",
    "#                  \"<account_user_name>\": [<list of timestamps in ms>]\n",
    "#             }\n",
    "#         ....\n",
    "#     },\n",
    "#     ....\n",
    "# }\n",
    "\n",
    "print(len(term_to_senders)) # should be 9998\n",
    "\n",
    "# prints slang ranked by frequency for a single user\n",
    "friend_name = \"Dan J Chong\"\n",
    "sender_to_term_count_fname = \"sender_to_term_count\"\n",
    "def return_zero():\n",
    "    return 0\n",
    "def return_dd():    \n",
    "    return defaultdict(return_zero)\n",
    "sender_to_term_count = pickle.load(open(sender_to_term_count_fname, \"rb\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# define terms for analysis\n",
    "msg_dir = \"/home/lhuang21/Documents/SideProjects/Grapevine/facebook-lawrenceh1850/messages/inbox\"\n",
    "account_user_name = \"Lawrence Huang\"\n",
    "# terms where I influenced friend\n",
    "# TERM = \"wtf\"\n",
    "# TERM = \"okay\"\n",
    "\n",
    "# 0 correlation\n",
    "# TERM = \"lol\"\n",
    "# TERM = \"ok\"\n",
    "\n",
    "# terms where friend influenced me\n",
    "# TERM = (\"holy\", \"shit\")\n",
    "# TERM = (\"shit\")\n",
    "# TERM = (\"lmao\")\n",
    "# TERM = (\"xd\")\n",
    "FRIEND_NAME = \"Dan J Chong\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     2,
     38
    ]
   },
   "outputs": [],
   "source": [
    "# calculate moving average\n",
    "\n",
    "def term_freq_avg(start_time_milli, end_time_milli, term, friend_name, msg_dir):\n",
    "    \"\"\"\n",
    "    Calculates average term frequency per word in a given time frame for all users involved in conversation.\n",
    "    \"\"\"\n",
    "    if start_time_milli > end_time_milli:\n",
    "        raise ValueError(\"End time must be after start time.\")\n",
    "    \n",
    "    dir_prefix = friend_name.strip().replace(\" \", \"\").lower()\n",
    "    msg_file_path = os.path.join(msg_dir, dir_prefix + \"*\")\n",
    "    candidate_dirs = glob.glob(msg_file_path)\n",
    "    \n",
    "    # this is for determining whether the term is a single word\n",
    "    # bigram or trigram\n",
    "    term_dimension = 1\n",
    "    if isinstance(term, tuple):\n",
    "        term_dimension = len(term)\n",
    "    \n",
    "    if len(candidate_dirs) != 1:\n",
    "        raise ValueError(\"Invalid path for friend name and message directory specified\")\n",
    "    else:\n",
    "        msg_file_path = os.path.join(candidate_dirs[0], \"message_1.json\") \n",
    "        \n",
    "        # make sure file exists\n",
    "        if not os.path.exists(msg_file_path):\n",
    "            raise ValueError(f\"{msg_file_path} doesn't exist\")\n",
    "        else:\n",
    "            json_dict = json.load(open(msg_file_path, 'r'))\n",
    "\n",
    "            word_count_in_period = {}\n",
    "            term_count_in_period = {}\n",
    "            \n",
    "            for name_dict in json_dict[\"participants\"]:\n",
    "                participant_name = name_dict[\"name\"]\n",
    "                term_count_in_period[participant_name] = 0\n",
    "                word_count_in_period[participant_name] = 0\n",
    "\n",
    "            for index, msg in enumerate(reversed(json_dict['messages'])):\n",
    "                msg_timestamp_ms = msg[\"timestamp_ms\"]\n",
    "                sender = msg[\"sender_name\"]\n",
    "                \n",
    "                if msg_timestamp_ms >= start_time_milli:\n",
    "                    if msg_timestamp_ms > end_time_milli:\n",
    "                        break\n",
    "                    \n",
    "                    try:\n",
    "                        content = preprocess(msg[\"content\"])\n",
    "                    except KeyError:\n",
    "                        # in case there isn't a content key\n",
    "                        continue\n",
    "                    tok_content = content.split()\n",
    "                    \n",
    "                    word_count_in_period[sender] += len(tok_content)\n",
    "                    \n",
    "                    if term_dimension == 1:\n",
    "                        # monograms\n",
    "                        for word in tok_content:\n",
    "                            word = word.strip(string.punctuation)\n",
    "                            if len(word) > 0:\n",
    "                                # only lemmatize if this was not a hand-picked word\n",
    "                                if word not in terms_to_include:\n",
    "                                    word = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "                                if word == term:\n",
    "                                    term_count_in_period[sender] += 1\n",
    "                    elif term_dimension == 2:\n",
    "                        # bigrams\n",
    "                        for (tok1, tok2) in ngrams(tok_content, 2):\n",
    "                            tok1 = tok1.strip(string.punctuation)\n",
    "                            tok2 = tok2.strip(string.punctuation)\n",
    "                            if term == (tok1, tok2):\n",
    "                                term_count_in_period[sender] += 1\n",
    "                    elif term_dimension == 3:\n",
    "                        # trigrams\n",
    "                        for (tok1, tok2, tok3) in ngrams(tok_content, 3):\n",
    "                            tok1 = tok1.strip(string.punctuation)\n",
    "                            tok2 = tok2.strip(string.punctuation)\n",
    "                            tok3 = tok3.strip(string.punctuation)\n",
    "                            if term == (tok1, tok2, tok3):\n",
    "                                term_count_in_period[sender] += 1\n",
    "                    else:\n",
    "                        raise ValueError(f\"Term has invalid input dimension\")\n",
    "                        \n",
    "            for sender in term_count_in_period:\n",
    "                if word_count_in_period[sender] != 0:\n",
    "                    term_count_in_period[sender] /= float(word_count_in_period[sender])\n",
    "                else:\n",
    "                    # sender had no words in this period\n",
    "                    term_count_in_period[sender] = 0\n",
    "                \n",
    "            return term_count_in_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# function that calculates term_to_usage_rates for friend\n",
    "\n",
    "# TODO: implement geometric weighting or some sort of moving average for earlier difference terms\n",
    "# maybe even just some sort of cutoff to account for noise\n",
    "# also the case where initial usage was relatively the same ad then later one person stopped \n",
    "# but the first person continued, maybe have to take into account the absolute value of the usage rate\n",
    "# like if it was 0 for both people at some point that's kind of an edge case (see \"kid\")\n",
    "def get_term_to_usage_rates_for_friend(FRIEND_NAME, top_x_terms=20):\n",
    "    term_to_usage_rates = {}\n",
    "    term_to_usage_rates[\"FRIEND_NAME\"] = FRIEND_NAME\n",
    "    # { \n",
    "    #   \"FRIEND_NAME\": \"\",\n",
    "    #   term_1: {\n",
    "    #     \"friend_usage_rates\": [ ... ],\n",
    "    #     \"user_usage_rates\": [ ... ],\n",
    "    #     \"x_axis\": x_axis\n",
    "    #   },\n",
    "    #   term_2 : {\n",
    "    #     ...\n",
    "    #   }\n",
    "    # ...\n",
    "    # }\n",
    "\n",
    "    term_counter = 0\n",
    "    \n",
    "    freq_per_friend = print_sorted_dict(sender_to_term_count[FRIEND_NAME], actually_print=False)\n",
    "    \n",
    "    for TERM in freq_per_friend:\n",
    "        print(f\"analyzing {TERM}\")\n",
    "        # only count the top x terms\n",
    "        if term_counter >= top_x_terms:\n",
    "            break\n",
    "        term_counter += 1\n",
    "            \n",
    "        friend_timestamps = term_to_senders[TERM][FRIEND_NAME][FRIEND_NAME]\n",
    "        user_timestamps = term_to_senders[TERM][FRIEND_NAME][account_user_name]\n",
    "\n",
    "        first_mention = None\n",
    "        last_mention = None\n",
    "        # no mentions\n",
    "        if len(friend_timestamps) == 0 and len(user_timestamps) == 0:\n",
    "            pass\n",
    "        elif len(friend_timestamps) == 0:\n",
    "            first_mention = user_timestamps[0]\n",
    "            last_mention = user_timestamps[0]\n",
    "        elif len(user_timestamps) == 0:\n",
    "            first_mention = friend_timestamps[0]\n",
    "            last_mention = friend_timestamps[0]\n",
    "        else:\n",
    "            first_mention = min(friend_timestamps[0], user_timestamps[0])\n",
    "            last_mention = max(friend_timestamps[-1], user_timestamps[-1])\n",
    "\n",
    "        # calculate usage rates across time period\n",
    "        if first_mention is not None and last_mention is not None:\n",
    "            x_axis = []\n",
    "            friend_usage_rates = []\n",
    "            user_usage_rates = []\n",
    "\n",
    "            START_MS = first_mention\n",
    "            END_MS = START_MS + MILLISEC_PER_MONTH\n",
    "\n",
    "            while START_MS <= last_mention:\n",
    "                rate_dict = term_freq_avg(START_MS, END_MS, TERM, FRIEND_NAME, msg_dir)\n",
    "                START_MS = END_MS\n",
    "                END_MS = START_MS + MILLISEC_PER_MONTH\n",
    "                x_axis.append(datetime.datetime.utcfromtimestamp(START_MS / 1000).replace(tzinfo=datetime.timezone.utc))\n",
    "                friend_usage_rates.append(rate_dict[FRIEND_NAME])\n",
    "                user_usage_rates.append(rate_dict[account_user_name])\n",
    "\n",
    "            term_to_usage_rates[TERM] = {\n",
    "                \"friend_usage_rates\": friend_usage_rates,\n",
    "                \"user_usage_rates\": user_usage_rates,\n",
    "                \"x_axis\": x_axis\n",
    "            }\n",
    "\n",
    "            # old dump to file\n",
    "            # pickle.dump(term_to_usage_rates, open(term_to_usage_rates_fname, \"wb\"))\n",
    "        else:\n",
    "            raise ValueError(f\"Should not reach here: No mentions of {TERM} found for {FRIEND_NAME}\")\n",
    "    \n",
    "    return term_to_usage_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     28
    ]
   },
   "outputs": [],
   "source": [
    "# write term_to_usage_rates to disk for every friend, don't redo if it already exists\n",
    "# TODO: make a nice input box for that\n",
    "\n",
    "friend_to_term_usage_rates_fname = \"friend_to_term_usage_rates\"\n",
    "friend_to_term_usage_rates = {}\n",
    "# { \n",
    "#   <FRIEND_NAME_1>: <term_to_usage_rates>,\n",
    "#   <FRIEND_NAME_2>: <term_to_usage_rates>,\n",
    "#   ...\n",
    "# }\n",
    "\n",
    "friend_to_words = pickle.load(open(\"friend_to_words\", \"rb\"))\n",
    "\n",
    "num_friends = 20\n",
    "print(f\"Top {num_friends} friends:\")\n",
    "sorted_friends = print_sorted_dict(friend_to_words, first_x=num_friends)\n",
    "\n",
    "redo_analysis = True\n",
    "if os.path.exists(friend_to_term_usage_rates_fname):\n",
    "    print(f\"\\n{friend_to_term_usage_rates_fname} dictionary exists already.\")\n",
    "    user_input = None\n",
    "    while user_input is None or not (user_input == \"y\" or user_input == \"n\"):\n",
    "        user_input = input(\"Redo analysis (y/n)? \").lower()\n",
    "    if user_input == \"y\":\n",
    "        redo_analysis = True\n",
    "    else:\n",
    "        redo_analysis = False\n",
    "\n",
    "if redo_analysis:\n",
    "    for FRIEND_NAME in sorted_friends[:num_friends]:\n",
    "        print(f\"Calculating usage rates for: {FRIEND_NAME}\")\n",
    "        term_to_usage_rates = get_term_to_usage_rates_for_friend(FRIEND_NAME)\n",
    "        friend_to_term_usage_rates[FRIEND_NAME] = term_to_usage_rates\n",
    "\n",
    "        pickle.dump(friend_to_term_usage_rates, open(friend_to_term_usage_rates_fname, \"wb\"))\n",
    "        print(f\"{friend_to_term_usage_rates_fname} dumped to disk\")\n",
    "else:\n",
    "    print(f\"{friend_to_term_usage_rates_fname} loaded from disk\")\n",
    "    friend_to_term_usage_rates = pickle.load(open(friend_to_term_usage_rates_fname, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     93,
     150
    ]
   },
   "outputs": [],
   "source": [
    "# loads term_to_usage_rates from disk and calculate influence scores\n",
    "\n",
    "friend_to_influence_scores = {}\n",
    "friend_to_influence_scores_fname = \"friend_to_influence_scores\"\n",
    "\n",
    "redo_analysis = True\n",
    "if os.path.exists(friend_to_term_usage_rates_fname):\n",
    "    print(f\"\\n{friend_to_influence_scores_fname} dictionary exists already.\")\n",
    "    user_input = None\n",
    "    while user_input is None or not (user_input == \"y\" or user_input == \"n\"):\n",
    "        user_input = input(\"Redo analysis (y/n)? \").lower()\n",
    "    if user_input == \"y\":\n",
    "        redo_analysis = True\n",
    "    else:\n",
    "        redo_analysis = False\n",
    "\n",
    "if redo_analysis:\n",
    "    for FRIEND_NAME in friend_to_term_usage_rates:\n",
    "        term_to_usage_rates = friend_to_term_usage_rates[FRIEND_NAME]\n",
    "        # { \n",
    "        #   \"FRIEND_NAME\": \"\",\n",
    "        #   term_1: {\n",
    "        #     \"friend_usage_rates\": [ ... ],\n",
    "        #     \"user_usage_rates\": [ ... ],\n",
    "        #     \"x_axis\": x_axis\n",
    "        #   },\n",
    "        #   term_2 : {\n",
    "        #     ...\n",
    "        #   }\n",
    "        # ...\n",
    "        # }\n",
    "        term_to_influence_score = {}\n",
    "\n",
    "        for TERM in term_to_usage_rates:\n",
    "            if TERM == \"FRIEND_NAME\":\n",
    "                print(f\"Friend name: {FRIEND_NAME}\")\n",
    "                continue\n",
    "\n",
    "            friend_usage_rates = np.asarray(term_to_usage_rates[TERM][\"friend_usage_rates\"])\n",
    "            user_usage_rates = np.asarray(term_to_usage_rates[TERM][\"user_usage_rates\"])\n",
    "            x_axis = np.asarray(term_to_usage_rates[TERM][\"x_axis\"])\n",
    "\n",
    "            # added to enable code folding\n",
    "            if True:\n",
    "                # difference: negative means friend influenced user\n",
    "                difference = user_usage_rates - friend_usage_rates\n",
    "\n",
    "                # Create the base line\n",
    "                start = x_axis[0]\n",
    "                stop = x_axis[-1]\n",
    "\n",
    "                # timestamps in seconds\n",
    "                x_timestamps = np.array([dt.replace(tzinfo=timezone.utc).timestamp() for dt in x_axis])\n",
    "\n",
    "                # time range covered by data\n",
    "                total_time = np.ptp(x_timestamps)\n",
    "                avg_diff = np.mean(abs(difference))\n",
    "\n",
    "                slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x=x_timestamps, y=difference)\n",
    "                user_slope, user_intercept, _, _, _ = scipy.stats.linregress(x=x_timestamps, y=user_usage_rates)\n",
    "                friend_slope, friend_intercept, _, _, _ = scipy.stats.linregress(x=x_timestamps, y=friend_usage_rates)\n",
    "\n",
    "                # make all difference values positive\n",
    "                difference_iqr = scipy.stats.iqr(difference)\n",
    "\n",
    "                # make sure this value is positive\n",
    "                if difference_iqr > 0:\n",
    "                    # the intercept is the value all the way at unix time 0, not when starting\n",
    "                    # this measure the base difference rate\n",
    "                    base_difference_rate = np.mean(difference[:max(len(difference // 10), 1)])\n",
    "\n",
    "                    # normalize the scope to calculate influence score\n",
    "                    # divide by difference_iqr to normalize values \n",
    "                    # multiply by absolute value of base_difference rate to account for whether there is a \n",
    "                    # initial difference\n",
    "                    \n",
    "                    influence_score = -slope * total_time / abs(np.mean(difference - min(difference))) * abs(base_difference_rate)\n",
    "\n",
    "        #             tried doing base rates from intercept, not accurate, switching to average over first 10th\n",
    "        #             user_base_rate = user_intercept + x_timestamps[0] * user_slope\n",
    "        #             friend_base_rate = friend_intercept + x_timestamps[0] * friend_slope\n",
    "\n",
    "                    user_base_rate = np.mean(user_usage_rates[:max(len(user_usage_rates // 10), 1)])\n",
    "                    friend_base_rate = np.mean(friend_usage_rates[:max(len(friend_usage_rates // 10), 1)])\n",
    "\n",
    "                    term_to_influence_score[TERM] = {\"score\": influence_score, \n",
    "                                                     \"user_base_rate\": user_base_rate, \n",
    "                                                     \"friend_base_rate\": friend_base_rate}\n",
    "\n",
    "                    # added to enable code folding\n",
    "#                     graph = True\n",
    "                    graph = False\n",
    "                    if graph:\n",
    "                        # 0 slope means there was little influence\n",
    "                        print(f\"TERM: {TERM}\")\n",
    "                        print(f\"Influence score: {influence_score}\")\n",
    "                        print(\"user_base_rate:\", user_base_rate)\n",
    "                        print(\"friend_base_rate:\", friend_base_rate)\n",
    "                        print(\"base_difference_rate:\", base_difference_rate)\n",
    "        #                 print(f\"Your usage rate slope: {user_slope}\")\n",
    "        #                 print(f\"Their usage rate slope: {friend_slope}\")\n",
    "        #                 print(f\"Diff slope: {slope}\")\n",
    "        #                 print(f\"Diff IQR: {difference_iqr}\")\n",
    "\n",
    "                        print(\"\"\"positive influence score = you influenced the other person\\nnegative influence score = the other person influenced you.\"\"\")\n",
    "                        # (aka your usage rate was initially higher than the other person)\n",
    "\n",
    "                        # negative influence score = the other person influenced you.\n",
    "                        # (aka your usage rate was initially lower than the other person\n",
    "\n",
    "                        fig, ax = plt.subplots(nrows=2, figsize=(25, 5))\n",
    "\n",
    "                        # TODO: set the y_lim dynamically, works well enough for now\n",
    "        #                 for i in range(len(ax)):\n",
    "        #                     ax[i].set_ylim(bottom=-0.03, top=0.03)\n",
    "\n",
    "                        ax[0].plot((start, stop), (0, 0), 'k', alpha=.5)\n",
    "                        ax[1].plot((start, stop), (0, 0), 'k', alpha=.5)\n",
    "\n",
    "                        ax[0].set(title=f\"Usage rates of {TERM} with {FRIEND_NAME}\")\n",
    "                        ax[0].plot(x_axis, friend_usage_rates, label=f\"{FRIEND_NAME}\", c='r')\n",
    "                        ax[0].plot(x_axis, friend_intercept + friend_slope*x_timestamps, c='orange')\n",
    "                        ax[0].plot(x_axis, user_usage_rates, label=f\"{account_user_name}\", c='b')\n",
    "                        ax[0].plot(x_axis, user_intercept + user_slope*x_timestamps, c='green')\n",
    "\n",
    "        #                 plot user base rate as sanity check\n",
    "        #                 ax[0].plot((x_axis[0]), (user_base_rate), 'ko', alpha=1, label=\"int\")\n",
    "\n",
    "\n",
    "                        ax[1].plot(x_axis, difference, label=\"Difference\", c='green')\n",
    "                        ax[1].plot(x_axis, intercept + slope*x_timestamps, c='red')\n",
    "\n",
    "                        for i in range(len(ax)):\n",
    "                            ax[i].get_xaxis().set_major_locator(mdates.MonthLocator(interval=1))\n",
    "                            ax[i].get_xaxis().set_major_formatter(mdates.DateFormatter(\"%b %Y\"))\n",
    "                            fig.autofmt_xdate()\n",
    "\n",
    "                            plt.setp((ax[i].get_yticklabels() + ax[i].get_yticklines() +\n",
    "                                          list(ax[i].spines.values())), visible=False)\n",
    "\n",
    "                            ax[i].legend(loc='upper left');\n",
    "                        plt.show()\n",
    "\n",
    "        # TODO: graph the z-scores\n",
    "        normalized_scores = np.asarray([term_to_influence_score[x][\"score\"] for x in term_to_influence_score])\n",
    "        normalized_scores = scipy.stats.zscore(normalized_scores)\n",
    "        counter = 0\n",
    "\n",
    "        total_term_count = 0\n",
    "        for TERM in term_to_influence_score:\n",
    "            total_term_count += sender_to_term_count[friend_name][TERM]\n",
    "\n",
    "        # print results\n",
    "        for TERM in term_to_influence_score:\n",
    "            # don't want to use the normalized scores for viz because normalizing the numbers\n",
    "            # causes them to lose their sign, and the sign tells you the direction of influence\n",
    "            # what you actually want to do is compare the standard deviations between different\n",
    "            # people, so I am just printing out the raw influence scores for now\n",
    "            cur_term_score = normalized_scores[counter]\n",
    "            counter += 1\n",
    "\n",
    "            # normalize by what the affected person's initial usage rate was,\n",
    "            # if the affected person's initial usage rate was high, the influence score should be low\n",
    "            # if the affected person's initial usage rate was low, the influence score should be high\n",
    "\n",
    "            # weight by term count? this doesn't seem to be effective either when multiplying or dividing it\n",
    "            # varies on a case by case\n",
    "    #         term_to_influence_score[TERM][\"score\"] *= sender_to_term_count[friend_name][TERM] * total_term_count\n",
    "\n",
    "            print(f'{TERM}: {term_to_influence_score[TERM][\"score\"]}')\n",
    "\n",
    "        friend_to_influence_scores[FRIEND_NAME] = term_to_influence_score\n",
    "    pickle.dump(friend_to_influence_scores, open(friend_to_influence_scores_fname, \"wb\"))\n",
    "else:\n",
    "    friend_to_influence_scores = pickle.load(open(friend_to_influence_scores_fname, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(friend_to_influence_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
