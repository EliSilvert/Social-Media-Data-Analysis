{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from collections import defaultdict, deque\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import statistics\n",
    "from nltk.util import ngrams\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import words\n",
    "from collections import Counter\n",
    "import operator\n",
    "import pprint\n",
    "import datetime\n",
    "import matplotlib.dates as mdates\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "def notin(target, comparelist):\n",
    "    for element in comparelist:\n",
    "        if element in target:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def preprocess(words):\n",
    "    words = words.lower()\n",
    "    # weird thing with apostrophe showing up as this unicode string\n",
    "    words = re.sub(\"\\u00e2\\u0080\\u0099\", \"'\", words)\n",
    "    # filter out all remaining Unicode, TODO: maybe want to translate these into emojis?\n",
    "    words = words.encode(\"ascii\", \"ignore\").decode(\"UTF-8\")\n",
    "    return words\n",
    "\n",
    "def print_sorted_dict(d, first_x=None, actually_print=True):\n",
    "    sorted_keys = sorted(d, key=d.get, reverse=True)\n",
    "    for i, key in enumerate(sorted_keys):\n",
    "        if first_x is not None and i >= first_x:\n",
    "            break\n",
    "        if actually_print:\n",
    "            print(f\"Rank {i+1}:\", key, d[key])\n",
    "    return sorted_keys\n",
    "\n",
    "def is_contraction(word):\n",
    "    if \"'\" in word:\n",
    "        split_word = word.split(\"'\")\n",
    "        if len(split_word) > 1 and split_word[0] in english_words:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "terms_to_include = set([\"kys\"])\n",
    "def exclude_word(word):\n",
    "    return (len(word) < 1 or word in english_words or word.isdigit() \n",
    "            or is_contraction(word) or word in standard_contractions) and (word not in terms_to_include)\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "def print_term_to_senders():\n",
    "    pp.pprint(term_to_senders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "# define some utility sets\n",
    "\n",
    "standard_contractions = set([\"aren't\", \"can't\", \"could've\", \"couldn't\", \"didn't\", \n",
    "                \"doesn't\", \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"he'd\", \"he'll\", \"he's\", \n",
    "                \"how'd\", \"how'll\", \"how's\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"isn't\", \"it'd\", \n",
    "                \"it'll\", \"it's\", \"let's\", \"might've\", \"mightn't\", \"must've\", \n",
    "                \"mustn't\", \"needn't\", \"o'clock\", \"she'd\", \"she'll\", \"she's\", \n",
    "                \"should've\", \"shouldn't\", \"that'd\", \"that's\", \"there'd\", \n",
    "                \"there's\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"wasn't\", \n",
    "                \"we'd\", \"we'll\", \"we're\", \"we've\", \"weren't\", \"what'll\", \"what're\", \"what's\",\n",
    "                \"what've\", \"when's\", \"when've\", \"where'd\", \"where's\", \"where've\", \"who'll\", \n",
    "                \"who's\", \"who've\", \"why's\", \"won't\", \"would've\", \n",
    "                \"wouldn't\", \"you'd\", \"you'll\", \"you're\", \"you've\"])\n",
    "\n",
    "# add in all contractions without the apostrophes also\n",
    "new_contractions = set()\n",
    "for contraction in standard_contractions:\n",
    "    new_contractions.add(contraction.replace(\"'\", \"\"))\n",
    "\n",
    "standard_contractions = standard_contractions.union(new_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "# define lemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# sample use cases\n",
    "# 1. Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 2. Lemmatize Single Word with the appropriate POS tag\n",
    "# word = 'feet'\n",
    "# print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "\n",
    "# 3. Lemmatize a Sentence with the appropriate POS tag\n",
    "# sentence = \"The striped bats are hanging on their feet for best\"\n",
    "# print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# extract message dictionaries\n",
    "\n",
    "msg_dir = \"/home/lhuang21/Documents/SideProjects/Grapevine/facebook-lawrenceh1850/messages/inbox\"\n",
    "account_user_name = \"Lawrence Huang\"\n",
    "\n",
    "# list of message dictionaries of interest\n",
    "conv_dicts = {}\n",
    "friend_to_num_msgs = defaultdict(lambda: 0)\n",
    "\n",
    "for dir_name in os.listdir(msg_dir):\n",
    "    if not dir_name.startswith(\".\"):\n",
    "        file_path = os.path.join(msg_dir, dir_name, 'message_1.json')\n",
    "        \n",
    "        # make sure file exists\n",
    "        if os.path.exists(file_path):\n",
    "            json_dict = json.load(open(file_path, 'r'))\n",
    "\n",
    "            # only look at DM's with more than 10 messages\n",
    "            if len(json_dict['participants']) == 2 and len(json_dict['messages']) > 10:\n",
    "                for friend_dict in json_dict['participants']:\n",
    "                    friend_name = friend_dict[\"name\"]\n",
    "                    if friend_name != account_user_name:\n",
    "                        num_msgs = len(json_dict['messages'])\n",
    "                        friend_to_num_msgs[friend_name] = num_msgs\n",
    "\n",
    "# print counts of individual messages\n",
    "sorted_names = print_sorted_dict(friend_to_num_msgs)\n",
    "\n",
    "plt.bar(list(range(len(sorted_names))), [friend_to_num_msgs[name] for name in sorted_names])\n",
    "    \n",
    "# top ten percentile of friends        \n",
    "top_ten_percentile = sorted_names[:len(sorted_names) // 10]\n",
    "\n",
    "for dir_name in os.listdir(msg_dir):\n",
    "    if not dir_name.startswith(\".\"):\n",
    "        file_path = os.path.join(msg_dir, dir_name, 'message_1.json')\n",
    "        \n",
    "        # make sure file exists\n",
    "        if os.path.exists(file_path):\n",
    "            json_dict = json.load(open(file_path, 'r'))\n",
    "\n",
    "            # only look at DM's with more than 10 messages\n",
    "            if len(json_dict['participants']) == 2 and len(json_dict['messages']) > 10:\n",
    "                for friend_dict in json_dict['participants']:\n",
    "                    friend_name = friend_dict[\"name\"]\n",
    "                    if friend_name != account_user_name and friend_name in top_ten_percentile:\n",
    "                        print(friend_name)\n",
    "                        conv_dicts[friend_name] = json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# counts number of words\n",
    "def return_zero():\n",
    "    return 0\n",
    "friend_to_words = defaultdict(return_zero)\n",
    "\n",
    "for friend_name, conv_dict in conv_dicts.items():\n",
    "#     print(\"Friend name:\", friend_name)\n",
    "#     print(\"=\" * 50)\n",
    "    \n",
    "    messages = conv_dict[\"messages\"]\n",
    "    for (index, msg) in enumerate(messages):\n",
    "        try:\n",
    "            content = msg['content']\n",
    "            tok_content = content.split()\n",
    "            friend_to_words[friend_name] += len(tok_content)\n",
    "        except KeyError:\n",
    "            pass\n",
    "_ = print_sorted_dict(friend_to_words, first_x=25)\n",
    "pickle.dump(friend_to_words, open(\"friend_to_words\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     6,
     45,
     47
    ]
   },
   "outputs": [],
   "source": [
    "# monogram/trigram/bigram extraction\n",
    "\n",
    "from nltk.corpus import words\n",
    "import pickle \n",
    "import os\n",
    "\n",
    "def add_term_timestamp(term_dict, term, friend_name, sender_name, timestamp_ms):\n",
    "    # figure out which person didn't send it\n",
    "    non_sender = friend_name\n",
    "    if friend_name == sender_name:\n",
    "        non_sender = account_user_name\n",
    "\n",
    "    # term doesn't exist in dict\n",
    "    if term_dict.get(term) is None:\n",
    "        term_dict[term] = {\n",
    "            friend_name: {\n",
    "                sender_name: deque([timestamp_ms]),\n",
    "                non_sender: deque([])\n",
    "            }\n",
    "        }\n",
    "    else: # term exists in dict\n",
    "        if term_dict[term].get(friend_name) is None: # not yet found in this conversation\n",
    "            term_dict[term][friend_name] = { sender_name: deque([timestamp_ms]), non_sender: deque([]) }\n",
    "        else:\n",
    "            # prepend because the file starts from the newest messages\n",
    "            term_dict[term][friend_name][sender_name].appendleft(timestamp_ms)\n",
    "\n",
    "pickled_dict_fname = \"term_to_senders_dict\"\n",
    "redo_analysis = True\n",
    "\n",
    "term_to_senders = {}\n",
    "# {\n",
    "#     \"<term>\": {\n",
    "#         \"<friend_name>\": \n",
    "#             {\n",
    "#                  \"<friend_name>\": [<list of timestamps in ms>],\n",
    "#                  \"<account_user_name>\": [<list of timestamps in ms>]\n",
    "#             }\n",
    "#         ....\n",
    "#     },\n",
    "#     ....\n",
    "# }\n",
    "sender_to_term_count_fname = \"sender_to_term_count\"\n",
    "\n",
    "# helper methods to allow sender_to_term_count to be pickle-able\n",
    "def return_zero():\n",
    "    return 0\n",
    "def return_dd():    \n",
    "    return defaultdict(return_zero)\n",
    "\n",
    "sender_to_term_count = defaultdict(return_dd)\n",
    "# {\n",
    "#   \"<user_name>\"\": {\n",
    "#     \"<term>\": count,\n",
    "#     ...\n",
    "#   }\n",
    "# }\n",
    "\n",
    "if os.path.exists(pickled_dict_fname):\n",
    "    print(\"Term to senders dictionary exists already.\")\n",
    "    user_input = None\n",
    "    while user_input is None or not (user_input == \"y\" or user_input == \"n\"):\n",
    "        user_input = input(\"Redo analysis (y/n)? \").lower()\n",
    "    if user_input == \"y\":\n",
    "        redo_analysis = True\n",
    "    else:\n",
    "        redo_analysis = False\n",
    "\n",
    "if not redo_analysis:\n",
    "    print(\"Loading dictionaries from file...\")\n",
    "    term_to_senders = pickle.load(open(pickled_dict_fname, \"rb\")) \n",
    "    sender_to_term_count = pickle.load(open(sender_to_term_count_fname, \"rb\")) \n",
    "    print(\"Dictionaries loaded\")\n",
    "else:\n",
    "    print(\"Redoing analysis...\")\n",
    "    \n",
    "    #  get monogram/bigram/trigrams\n",
    "    english_words = set(words.words())\n",
    "\n",
    "    for friend_name, conv_dict in conv_dicts.items():\n",
    "        print(\"Conversation with\", friend_name)\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        messages = conv_dict[\"messages\"]\n",
    "        for (index, msg) in enumerate(messages): \n",
    "            try:\n",
    "                sender_name = msg[\"sender_name\"]\n",
    "                content = msg[\"content\"]\n",
    "                timestamp_ms = msg[\"timestamp_ms\"]\n",
    "\n",
    "                if notin(content, [\"You are now connected on Messenger.\", \"Say hi to your new Facebook friend\"]):\n",
    "                    content = preprocess(content)\n",
    "\n",
    "    #                 alternate ways of tokenizing\n",
    "    #                 tok_content = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(content)]\n",
    "    #                 tok_content = nltk.word_tokenize(content)\n",
    "\n",
    "                    tok_content = content.split()\n",
    "\n",
    "                    # monograms\n",
    "                    for word in tok_content:\n",
    "                        word = word.strip(string.punctuation)\n",
    "                        if len(word) > 0:\n",
    "                            # only lemmatize if this was not a hand-picked word\n",
    "                            if word not in terms_to_include: \n",
    "                                word = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "                            if not exclude_word(word):\n",
    "                                add_term_timestamp(term_to_senders, word, friend_name, sender_name, timestamp_ms)\n",
    "\n",
    "                    # bigrams\n",
    "                    for (tok1, tok2) in ngrams(tok_content, 2):\n",
    "                        tok1 = tok1.strip(string.punctuation)\n",
    "                        tok2 = tok2.strip(string.punctuation)\n",
    "                        if (exclude_word(tok1) and exclude_word(tok2)\n",
    "                            or (tok1.isdigit() or tok2.isdigit())):\n",
    "                            continue\n",
    "                        else:\n",
    "                            add_term_timestamp(term_to_senders, (tok1, tok2), friend_name, sender_name, timestamp_ms)\n",
    "\n",
    "                    # trigrams\n",
    "                    for (tok1, tok2, tok3) in ngrams(tok_content, 3):\n",
    "                        tok1 = tok1.strip(string.punctuation)\n",
    "                        tok2 = tok2.strip(string.punctuation)\n",
    "                        tok3 = tok3.strip(string.punctuation)\n",
    "                        num_stop = 0\n",
    "                        if exclude_word(tok1):\n",
    "                            num_stop += 1\n",
    "                        if exclude_word(tok2):\n",
    "                            num_stop += 1\n",
    "                        if exclude_word(tok3):\n",
    "                            num_stop += 1\n",
    "\n",
    "                        if num_stop >= 2:\n",
    "                            continue\n",
    "                        else:\n",
    "                            add_term_timestamp(term_to_senders, (tok1, tok2, tok3), friend_name, sender_name, timestamp_ms)\n",
    "            except KeyError:\n",
    "                # messages that are just photos don't have a content key\n",
    "                pass\n",
    "    \n",
    "    # FILTER term_to_senders based on occurrence frequency\n",
    "    # TODO: sort by occurrence, and see the cutoff filtering based on something besides frequency\n",
    "\n",
    "    terms_to_delete = set()\n",
    "    term_to_count = defaultdict(lambda: 0)\n",
    "\n",
    "    for term in term_to_senders:\n",
    "        num_usages = 0\n",
    "        for friend_name in term_to_senders[term]:\n",
    "            # count up usages for both user and friends\n",
    "            for user_name in term_to_senders[term][friend_name]:\n",
    "                timestamp_count = len(term_to_senders[term][friend_name][user_name])\n",
    "                num_usages += timestamp_count\n",
    "                sender_to_term_count[user_name][term] += timestamp_count\n",
    "        if num_usages < 4:\n",
    "            terms_to_delete.add(term)\n",
    "        else:\n",
    "            term_to_count[term] += num_usages\n",
    "        for sender in sender_to_term_count:\n",
    "            try:\n",
    "                if sender_to_term_count[sender][term] < 4:\n",
    "                    del sender_to_term_count[sender][term]\n",
    "            except KeyError:\n",
    "                pass\n",
    "    for term in terms_to_delete:\n",
    "         del term_to_senders[term]\n",
    "    print(len(term_to_senders)) # should be 9998\n",
    "    \n",
    "    print(\"Saving dictionary to file\")\n",
    "    pickle.dump(term_to_senders, open(pickled_dict_fname, \"wb\"))\n",
    "    pickle.dump(sender_to_term_count, open(sender_to_term_count_fname, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# prints slang ranked by frequency for a single user\n",
    "friend_name = \"Dan J Chong\"\n",
    "freq_per_friend = print_sorted_dict(sender_to_term_count[friend_name])\n",
    "# print_sorted_dict(sender_to_term_count[\"Lawrence Huang\"])\n",
    "\n",
    "# prints slang ranked by frequency across all users\n",
    "# _ = print_sorted_dict(term_to_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_term_mentions(term_examined, friend_name):\n",
    "    # DO NOT MODIFY\n",
    "    # =================================\n",
    "    account_user_name = \"Lawrence Huang\"\n",
    "    # =================================\n",
    "    sample_dict = term_to_senders[term_examined][friend_name]\n",
    "\n",
    "    other_mentions = [datetime.datetime.utcfromtimestamp(i / 1000).replace(tzinfo=datetime.timezone.utc) for i in sample_dict[friend_name]]\n",
    "    me_mentions = [datetime.datetime.utcfromtimestamp(i / 1000).replace(tzinfo=datetime.timezone.utc) for i in sample_dict[account_user_name]]\n",
    "#     print(len(other_mentions + me_mentions))\n",
    "    # This tutorial as reference: https://matplotlib.org/3.0.0/gallery/lines_bars_and_markers/timeline.html\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(25, 5))\n",
    "\n",
    "    # Create the base line\n",
    "    start = min(me_mentions + other_mentions)\n",
    "    stop = max(me_mentions + other_mentions)\n",
    "    ax.plot((start, stop), (0, 0), 'k', alpha=.5)\n",
    "\n",
    "    # Iterate through releases annotating each one\n",
    "    for i, date in enumerate(me_mentions):\n",
    "        ax.plot((date, date), (0, 1), c='r', alpha=.7, label=f\"{account_user_name}\" if i == 0 else \"_nolegend_\")\n",
    "    for i, date in enumerate(other_mentions):\n",
    "        # Plot a line up to the text\n",
    "        ax.plot((date, date), (0, -1), c='b', alpha=.7, label=f\"{friend_name}\" if i == 0 else \"_nolegend_\")\n",
    "\n",
    "    ax.set(title=f\"Mentions of {term_examined} with {friend_name}\")\n",
    "    # Set the xticks formatting\n",
    "    # TODO: make the month interval calculation dynamic based on difference between start and stop\n",
    "    # format xaxis with x month intervals\n",
    "    ax.get_xaxis().set_major_locator(mdates.MonthLocator(interval=1))\n",
    "    ax.get_xaxis().set_major_formatter(mdates.DateFormatter(\"%b %Y\"))\n",
    "    fig.autofmt_xdate()\n",
    "\n",
    "    plt.legend(loc='upper left');\n",
    "\n",
    "    # Remove components for a cleaner look\n",
    "    plt.setp((ax.get_yticklabels() + ax.get_yticklines() +\n",
    "              list(ax.spines.values())), visible=False)\n",
    "    plt.show()\n",
    "\n",
    "for term_examined in freq_per_friend:\n",
    "    plot_term_mentions(term_examined, friend_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY THESE\n",
    "# =================================\n",
    "term_examined = \"\"\n",
    "friend_name = \"\"\n",
    "plot_term_mentions(term_examined, friend_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
